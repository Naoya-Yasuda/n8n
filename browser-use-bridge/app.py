#!/usr/bin/env python3
"""
Improved browser-use-bridge using direct Bedrock Converse API
"""

from fastapi import FastAPI, HTTPException
from browser_use import Agent, Browser, BrowserConfig
from browser_use.llm import BaseChatModel, UserMessage, AssistantMessage
import asyncio
import uuid
import os
import boto3
import aiohttp
from typing import Dict, List
from pydantic import BaseModel
from dotenv import load_dotenv
import json

# LangChainãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚¯ãƒ©ã‚¹ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from langchain_core.messages import AIMessage
    LANGCHAIN_AVAILABLE = True
except ImportError:
    LANGCHAIN_AVAILABLE = False
    # ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚¯ãƒ©ã‚¹ã‚’å®šç¾©
    class AIMessage:
        def __init__(self, content: str):
            self.content = content
            self.type = "ai"

app = FastAPI()
load_dotenv()

# ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«
class TaskRequest(BaseModel):
    task: str
    description: str = ""
    webhook_url: str = ""

# ã‚¿ã‚¹ã‚¯ç®¡ç†
tasks: Dict[str, dict] = {}

class DirectBedrockLLM(BaseChatModel):
    """OpenAIã‚¹ã‚¿ã‚¤ãƒ«å®Ÿè£…ã®DirectBedrockLLM - output_format.model_validate_json()ã‚’ä½¿ç”¨"""
    
    def __init__(self, model_id: str = "apac.anthropic.claude-3-5-sonnet-20241022-v2:0", region: str = "ap-northeast-1"):
        super().__init__()
        self.model_id = model_id
        self.model = model_id  # å¿…é ˆ - browser-useã®ãƒˆãƒ¼ã‚¯ãƒ³ç®¡ç†ã§ä½¿ç”¨
        self.region = region
        self._setup_client()
        print(f"ğŸ¯ DirectBedrockLLM initialized with OpenAI-style implementation: {model_id}")
    
    @property
    def _llm_type(self) -> str:
        return "direct_bedrock_converse"
    
    def _setup_client(self):
        session_kwargs = {
            'aws_access_key_id': os.environ.get('AWS_ACCESS_KEY_ID'),
            'aws_secret_access_key': os.environ.get('AWS_SECRET_ACCESS_KEY'),
            'region_name': self.region
        }
        aws_session_token = os.environ.get('AWS_SESSION_TOKEN')
        if aws_session_token:
            session_kwargs['aws_session_token'] = aws_session_token
        session = boto3.Session(**session_kwargs)
        self.client = session.client('bedrock-runtime')
    
    def _convert_messages(self, messages: list) -> tuple:
        """Convert browser-use messages to Bedrock format"""
        bedrock_messages = []
        system_prompt = ""
        
        for msg in messages:
            if hasattr(msg, 'type'):
                role = msg.type
                content = msg.content
            elif isinstance(msg, dict):
                role = msg.get('role', 'user')
                content = msg.get('content', '')
            else:
                role = 'user'
                content = str(msg)
            
            if role == 'system':
                system_prompt = str(content)
                continue
            
            bedrock_role = 'user' if role in ['user', 'human'] else 'assistant'
            bedrock_messages.append({
                "role": bedrock_role,
                "content": [{"text": str(content)}]
            })
        
        return bedrock_messages, system_prompt
    
    async def ainvoke(self, messages: list, output_format=None):
        """OpenAIã‚¹ã‚¿ã‚¤ãƒ«ã®ainvokeå®Ÿè£… - output_format.model_validate_json()ã‚’ä½¿ç”¨"""
        print(f"ğŸ§  DirectBedrockLLM.ainvoke called with {len(messages)} messages, output_format={output_format is not None}")
        
        try:
            # Bedrockãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å¤‰æ›
            bedrock_messages, system_prompt = self._convert_messages(messages)
            
            if not bedrock_messages:
                bedrock_messages = [{"role": "user", "content": [{"text": "Hello"}]}]
            
            # Bedrock Converse APIå‘¼ã³å‡ºã—
            converse_kwargs = {
                'modelId': self.model_id,
                'messages': bedrock_messages,
                'inferenceConfig': {
                    'temperature': 0.1,
                    'maxTokens': 4096
                }
            }
            
            if system_prompt:
                converse_kwargs['system'] = [{"text": system_prompt}]
            
            response = await asyncio.get_event_loop().run_in_executor(
                None, 
                lambda: self.client.converse(**converse_kwargs)
            )
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
            output_message = response.get('output', {}).get('message', {})
            content = output_message.get('content', [])
            text_content = content[0]['text'] if content and content[0].get('text') else "No response"
            
            print(f"ğŸ“ Bedrock response: {text_content[:200]}...")
            
            # Usageæƒ…å ±ã®ä½œæˆ
            from browser_use.llm.views import ChatInvokeUsage
            usage_data = response.get('usage', {})
            usage = ChatInvokeUsage(
                prompt_tokens=usage_data.get('inputTokens', 0),
                completion_tokens=usage_data.get('outputTokens', 0),
                total_tokens=usage_data.get('inputTokens', 0) + usage_data.get('outputTokens', 0),
                prompt_cached_tokens=None,
                prompt_cache_creation_tokens=None,
                prompt_image_tokens=None
            )
            
            # OpenAIã¨åŒã˜ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
            if output_format is None:
                # output_formatãŒãªã„å ´åˆã¯æ–‡å­—åˆ—ã‚’è¿”ã™ï¼ˆOpenAIã¨åŒã˜ï¼‰
                from browser_use.llm.views import ChatInvokeCompletion
                return ChatInvokeCompletion(
                    completion=text_content,
                    usage=usage
                )
            else:
                # output_formatãŒã‚ã‚‹å ´åˆã¯Pydanticã§è§£æï¼ˆOpenAIã¨åŒã˜ï¼‰
                print("ğŸ”§ Using output_format.model_validate_json() - OpenAI style")
                parsed = output_format.model_validate_json(text_content)
                
                print(f"âœ… Successfully parsed to AgentOutput with {len(parsed.action) if parsed.action else 0} actions")
                
                from browser_use.llm.views import ChatInvokeCompletion
                return ChatInvokeCompletion(
                    completion=parsed,  # AgentOutputã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
                    usage=usage
                )
                
        except Exception as e:
            print(f"âŒ DirectBedrockLLM error: {e}")
            import traceback
            traceback.print_exc()
            
            # ã‚¨ãƒ©ãƒ¼æ™‚ã®Usage
            from browser_use.llm.views import ChatInvokeUsage, ChatInvokeCompletion
            error_usage = ChatInvokeUsage(
                prompt_tokens=0,
                completion_tokens=0,
                total_tokens=0,
                prompt_cached_tokens=None,
                prompt_cache_creation_tokens=None,
                prompt_image_tokens=None
            )
            
            if output_format is None:
                # æ–‡å­—åˆ—ã‚¨ãƒ©ãƒ¼ãƒ¬ã‚¹ãƒãƒ³ã‚¹
                return ChatInvokeCompletion(
                    completion=f"Error: {str(e)}",
                    usage=error_usage
                )
            else:
                # æœ€å°é™ã®AgentOutputã‚¨ãƒ©ãƒ¼ãƒ¬ã‚¹ãƒãƒ³ã‚¹
                try:
                    error_json = '{"thinking": "Error occurred", "evaluation_previous_goal": null, "memory": "Error state", "next_goal": "Navigate to MLex login page", "action": [{"go_to_url": {"url": "https://identity.mlex.com/Account/Login"}}]}'
                    error_parsed = output_format.model_validate_json(error_json)
                    
                    return ChatInvokeCompletion(
                        completion=error_parsed,
                        usage=error_usage
                    )
                except:
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æ–‡å­—åˆ—è¿”å´
                    return ChatInvokeCompletion(
                        completion=f"Error: {str(e)}",
                        usage=error_usage
                    )
    
    async def acomplete(self, messages: list, **kwargs):
        """Legacy acomplete method - delegates to ainvoke"""
        print(f"DEBUG: DirectBedrockLLM.acomplete called - delegating to ainvoke")
        return await self.ainvoke(messages, **kwargs)
    
    def complete(self, messages: list, **kwargs):
        """Sync completion method"""
        print(f"DEBUG: DirectBedrockLLM.complete called with {len(messages)} messages")
        return asyncio.run(self.ainvoke(messages, **kwargs))
    
    def __call__(self, *args, **kwargs):
        """Handle direct call"""
        print(f"DEBUG: DirectBedrockLLM.__call__ called with args={args}, kwargs={kwargs}")
        result = None
        try:
            if args and isinstance(args[0], list):
                result = self.complete(args[0], **kwargs)
            else:
                result = self.complete(*args, **kwargs)
            print(f"DEBUG: DirectBedrockLLM.__call__ returning: {type(result)}")
            return result
        except Exception as e:
            print(f"DEBUG: DirectBedrockLLM.__call__ error: {e}")
            raise
    
    async def __acall__(self, *args, **kwargs):
        """Handle async direct call"""
        print(f"DEBUG: DirectBedrockLLM.__acall__ called with args={args}, kwargs={kwargs}")
        result = None
        try:
            if args and isinstance(args[0], list):
                result = await self.ainvoke(args[0], **kwargs)
            else:
                result = await self.ainvoke(*args, **kwargs)
            print(f"DEBUG: DirectBedrockLLM.__acall__ returning: {type(result)}")
            return result
        except Exception as e:
            print(f"DEBUG: DirectBedrockLLM.__acall__ error: {e}")
            raise

@app.get("/health")
async def health_check():
    return {"status": "healthy", "implementation": "direct_bedrock_converse"}

@app.post("/api/v1/run-task")
async def run_task(request: TaskRequest):
    task_id = str(uuid.uuid4())

    print(f"DEBUG: Received task request - Task: {request.task}, Description: {request.description}")

    try:
        # Direct Bedrock LLM ã‚’ä½œæˆ
        llm = DirectBedrockLLM(
            model_id="apac.anthropic.claude-3-5-sonnet-20241022-v2:0",
            region="ap-northeast-1"
        )

        print(f"DEBUG: Created DirectBedrockLLM: {type(llm)}")
        print(f"DEBUG: DirectBedrockLLM.model: {llm.model}")
        print(f"DEBUG: DirectBedrockLLM._llm_type: {llm._llm_type}")
        print(f"DEBUG: Is BaseChatModel instance: {isinstance(llm, BaseChatModel)}")

        agent = Agent(
            task=request.task,
            llm=llm,
            browser=Browser(config=BrowserConfig(
                headless=True,
                browser_type="chromium"
            )),
            validate_output=False,  # Bedrockäº’æ›æ€§ã®ãŸã‚ç„¡åŠ¹åŒ–
            max_actions_per_step=3,  # ãƒ­ã‚°ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ã®ãŸã‚å¢åŠ 
            max_steps=15             # ã‚¹ãƒ†ãƒƒãƒ—æ•°ã‚’å¢—åŠ 
        )

        print(f"DEBUG: Agent created with LLM: {type(agent.llm)}")
        print(f"DEBUG: Agent LLM model: {getattr(agent.llm, 'model', 'NO_MODEL_ATTR')}")
        print(f"DEBUG: Agent LLM _llm_type: {getattr(agent.llm, '_llm_type', 'NO_LLM_TYPE_ATTR')}")

        print(f"DEBUG: Starting task {task_id} with Direct Bedrock Converse API")
        print(f"DEBUG: Task description: {request.task}")

        # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§ã‚¿ã‚¹ã‚¯å®Ÿè¡Œ
        asyncio.create_task(execute_task(task_id, agent, request.webhook_url))

        return {
            "task_id": task_id,
            "status": "running",
            "message": "ã‚¿ã‚¹ã‚¯ãŒé–‹å§‹ã•ã‚Œã¾ã—ãŸï¼ˆDirect Bedrock Converse APIä½¿ç”¨ï¼‰",
            "status_url": f"/api/v1/task/{task_id}/status",
            "llm_type": "direct_bedrock_converse"
        }

    except Exception as e:
        print(f"DEBUG: Task start error: {e}")
        return {
            "task_id": task_id,
            "status": "error",
            "message": f"ã‚¿ã‚¹ã‚¯é–‹å§‹ã‚¨ãƒ©ãƒ¼: {str(e)}"
        }

async def execute_task(task_id: str, agent, webhook_url: str = ""):
    start_time = asyncio.get_event_loop().time()
    screenshot_path = None
    try:
        tasks[task_id] = {
            "status": "running",
            "progress": "é–‹å§‹ä¸­...",
            "started_at": start_time,
            "elapsed_seconds": 0
        }

        # å®šæœŸçš„ã«ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹æ›´æ–°
        progress_task = asyncio.create_task(update_progress(task_id, start_time))

        print(f"DEBUG: Task {task_id} - Starting agent.run() with Direct Bedrock")
        result = await agent.run()
        print(f"DEBUG: Task {task_id} - agent.run() completed successfully")

        # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†å¾Œã«ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã‚’æ’®å½±
        try:
            print(f"ğŸ“¸ Taking final screenshot for task {task_id}")
            # browser-useã®Agentã‹ã‚‰ãƒ–ãƒ©ã‚¦ã‚¶ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’å–å¾—
            if hasattr(agent, 'browser') and hasattr(agent.browser, 'page'):
                import os
                screenshot_dir = "/tmp/screenshots"
                os.makedirs(screenshot_dir, exist_ok=True)
                screenshot_path = f"{screenshot_dir}/task_{task_id}_final.png"
                
                await agent.browser.page.screenshot(path=screenshot_path, full_page=True)
                print(f"âœ… Screenshot saved to: {screenshot_path}")
            else:
                print("âš ï¸ Browser instance not available for screenshot")
        except Exception as screenshot_error:
            print(f"âš ï¸ Screenshot capture failed: {screenshot_error}")

        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹æ›´æ–°ã‚¿ã‚¹ã‚¯ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«
        progress_task.cancel()

        end_time = asyncio.get_event_loop().time()
        
        # çµæœã«ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆãƒ‘ã‚¹ã‚’è¿½åŠ 
        result_data = {
            "agent_result": result,
            "screenshot_path": screenshot_path,
            "screenshot_available": screenshot_path is not None
        }
        
        tasks[task_id] = {
            "status": "completed",
            "result": result_data,
            "started_at": start_time,
            "completed_at": end_time,
            "total_duration_seconds": round(end_time - start_time, 2),
            "llm_type": "direct_bedrock_converse",
            "screenshot_path": screenshot_path
        }

        # webhook URLãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯é€šçŸ¥
        if webhook_url:
            asyncio.create_task(send_webhook(webhook_url, task_id, "completed", result_data))

    except Exception as e:
        end_time = asyncio.get_event_loop().time()
        error_details = {
            "error": str(e),
            "error_type": type(e).__name__,
            "error_message": str(e),
            "llm_type": "direct_bedrock_converse"
        }

        # ã‚ˆã‚Šè©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’è¿½åŠ 
        if hasattr(e, '__traceback__'):
            import traceback
            error_details["traceback"] = traceback.format_exc()

        tasks[task_id] = {
            "status": "failed",
            "error": str(e),
            "error_details": error_details,
            "started_at": start_time,
            "failed_at": end_time,
            "total_duration_seconds": round(end_time - start_time, 2)
        }

        print(f"DEBUG: Task {task_id} failed: {e}")

        # webhook URLãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯é€šçŸ¥
        if webhook_url:
            asyncio.create_task(send_webhook(webhook_url, task_id, "failed", error_details))

async def update_progress(task_id: str, start_time: float):
    """å®šæœŸçš„ã«ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹æƒ…å ±ã‚’æ›´æ–°"""
    try:
        while True:
            await asyncio.sleep(10)  # 10ç§’ã”ã¨ã«æ›´æ–°
            if task_id in tasks and tasks[task_id]["status"] == "running":
                elapsed = asyncio.get_event_loop().time() - start_time
                tasks[task_id]["elapsed_seconds"] = round(elapsed, 2)

                # çµŒéæ™‚é–“ã«å¿œã˜ã¦ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æ›´æ–°
                if elapsed < 60:
                    progress_msg = "åˆæœŸåŒ–ä¸­ï¼ˆDirect Bedrockï¼‰..."
                elif elapsed < 180:
                    progress_msg = "ãƒ–ãƒ©ã‚¦ã‚¶æ“ä½œå®Ÿè¡Œä¸­..."
                elif elapsed < 300:
                    progress_msg = "ãƒ‡ãƒ¼ã‚¿å‡¦ç†ä¸­..."
                else:
                    progress_msg = f"å®Ÿè¡Œä¸­... ({int(elapsed)}ç§’çµŒé)"

                tasks[task_id]["progress"] = progress_msg
    except asyncio.CancelledError:
        pass  # ã‚¿ã‚¹ã‚¯å®Œäº†æ™‚ã«ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã‚‹

@app.get("/api/v1/task/{task_id}/status")
async def get_task_status(task_id: str):
    if task_id not in tasks:
        raise HTTPException(status_code=404, detail="Task not found")

    task_info = tasks[task_id]

    # å®Œäº†ã¾ãŸã¯å¤±æ•—ã—ãŸã‚¿ã‚¹ã‚¯ã¯ä¸€å®šæ™‚é–“å¾Œã«å‰Šé™¤
    if task_info["status"] in ["completed", "failed"]:
        # 1æ™‚é–“å¾Œã«ã‚¿ã‚¹ã‚¯æƒ…å ±ã‚’å‰Šé™¤
        asyncio.create_task(cleanup_task(task_id, delay=3600))

    return task_info

async def send_webhook(webhook_url: str, task_id: str, status: str, data: dict):
    """webhookã‚’é€ä¿¡"""
    try:
        payload = {
            "task_id": task_id,
            "status": status,
            "timestamp": asyncio.get_event_loop().time(),
            "data": data
        }

        async with aiohttp.ClientSession() as session:
            async with session.post(webhook_url, json=payload) as response:
                if response.status == 200:
                    print(f"Webhook sent successfully to {webhook_url}")
                else:
                    print(f"Webhook failed with status {response.status}")
    except Exception as e:
        print(f"Webhook error: {e}")

async def cleanup_task(task_id: str, delay: int):
    """æŒ‡å®šæ™‚é–“å¾Œã«ã‚¿ã‚¹ã‚¯æƒ…å ±ã‚’å‰Šé™¤"""
    await asyncio.sleep(delay)
    if task_id in tasks:
        del tasks[task_id]

@app.get("/api/v1/tasks")
async def list_tasks():
    """å®Ÿè¡Œä¸­ã®ã‚¿ã‚¹ã‚¯ä¸€è¦§ã‚’å–å¾—"""
    return {
        "active_tasks": len([t for t in tasks.values() if t["status"] == "running"]),
        "completed_tasks": len([t for t in tasks.values() if t["status"] == "completed"]),
        "failed_tasks": len([t for t in tasks.values() if t["status"] == "failed"]),
        "total_tasks": len(tasks),
        "llm_implementation": "direct_bedrock_converse"
    }

# ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆå–å¾—ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
@app.get("/api/v1/task/{task_id}/screenshot")
async def get_screenshot(task_id: str):
    """ã‚¿ã‚¹ã‚¯ã®ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã‚’å–å¾—"""
    if task_id not in tasks:
        raise HTTPException(status_code=404, detail="Task not found")
    
    task_info = tasks[task_id]
    screenshot_path = task_info.get("screenshot_path")
    
    if not screenshot_path or not os.path.exists(screenshot_path):
        raise HTTPException(status_code=404, detail="Screenshot not available")
    
    from fastapi.responses import FileResponse
    return FileResponse(
        screenshot_path, 
        media_type="image/png", 
        filename=f"task_{task_id}_screenshot.png"
    )

# ãƒ†ã‚¹ãƒˆç”¨ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
@app.post("/api/v1/test-llm")
async def test_llm():
    """Direct Bedrock LLMæ¥ç¶šãƒ†ã‚¹ãƒˆ"""
    try:
        llm = DirectBedrockLLM(
            model_id="apac.anthropic.claude-3-5-sonnet-20241022-v2:0",
            region="ap-northeast-1"
        )
        
        # ç°¡å˜ãªãƒ†ã‚¹ãƒˆ
        messages = [{"role": "user", "content": "Hello, please respond with 'Direct Bedrock Converse working!'"}]
        response = await llm.ainvoke(messages)
        
        return {
            "status": "success",
            "llm_type": "direct_bedrock_converse",
            "response": str(response),
            "message": "Direct Bedrock LLMæ¥ç¶šãƒ†ã‚¹ãƒˆæˆåŠŸ"
        }
        
    except Exception as e:
        import traceback
        return {
            "status": "error",
            "llm_type": "direct_bedrock_converse",
            "error": str(e),
            "traceback": traceback.format_exc(),
            "message": "Direct Bedrock LLMæ¥ç¶šãƒ†ã‚¹ãƒˆå¤±æ•—"
        }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)  # æœ¬ç•ªãƒãƒ¼ãƒˆ8000ã‚’ä½¿ç”¨